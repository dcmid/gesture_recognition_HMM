{
 "cells": [
  {
   "source": [
    "# IAS Project 2 Development Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is the working notebook for development on IAS Project 2. All intermittent steps, experimental plots, etc used in developing and training an HMM for gesture recognition are here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import pickle as pkl\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "source": [
    "## HMM Filtering\n",
    "### Forward Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_filt(A, B, pi, obs):\n",
    "    K = len(obs) #number of observations\n",
    "    S = len(A) #number of states\n",
    "    #q_ki = P(s_k = i, y_1:k)\n",
    "    alpha = np.zeros((K,S))\n",
    "    scale_factor = np.zeros(K)\n",
    "\n",
    "    #initialization\n",
    "    alpha[0] = np.multiply(B[:,obs[0]],pi) #4x1 vector  \n",
    "    scale_factor[0] = 1/np.maximum(1e-300,np.sum(alpha[0]))\n",
    "    alpha[0] *= scale_factor[0] #normalizen\n",
    "\n",
    "    for k in range(1,K):\n",
    "        B_k = np.diag(B[:,obs[k]])\n",
    "        alpha[k] = np.dot(np.dot(B_k,A.T),alpha[k-1]) #P(j,o..) = P(i,o..)P(j|i)P(o|j)\n",
    "        #alpha[k][alpha[k] == 0] = 1e-10\n",
    "        scale_factor[k] = 1/np.maximum(1e-300,np.sum(alpha[k]))\n",
    "        alpha[k] = alpha[k] * scale_factor[k] #normalize\n",
    "\n",
    "    # #Equivalent For Loop\n",
    "    # for k in range(1,len(obs)):\n",
    "    #     for j in range(len(A)):\n",
    "    #         for i in range(len(A)):\n",
    "    #             alpha[k,j] += alpha[k-1,i]*A[i,j]*B[j,obs[k]]\n",
    "    #     scale_factor[k] = 1/np.sum(alpha[k])\n",
    "    #     alpha[k] = alpha[k] * scale_factor[k]\n",
    "\n",
    "    return [alpha, scale_factor]"
   ]
  },
  {
   "source": [
    "### Backward Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_filt(A, B, obs, scale_factor):\n",
    "    nstates = len(A)\n",
    "    K = len(obs)\n",
    "    beta = np.zeros((K,nstates))\n",
    "    #initialization\n",
    "    beta[K-1] = np.ones(nstates)\n",
    "    #beta[K-1][beta[K-1] == 0] = 1e-10\n",
    "    #beta[K-1] = beta[K-1]/len(A)\n",
    "    beta[K-1] *= scale_factor[K-1]\n",
    "    for k in range(K-1,0,-1):\n",
    "        B_k = np.diag(B[:,obs[k]]) #diagonalize for elementwise multiplication w/ dot prod\n",
    "        beta[k-1] = np.dot(np.dot(A,B_k),beta[k]) #beta[k-1] = A * B_k * beta[k]\n",
    "        beta[k-1] *= scale_factor[k-1]\n",
    "        #beta[k-1] = beta[k-1]/np.maximum(1e-300,np.sum(beta[k-1]))\n",
    "\n",
    "    # #Equivalent For Loop\n",
    "    # for k in range(K-1,0,-1):\n",
    "    #     for i in range(nstates):\n",
    "    #         for j in range(nstates):\n",
    "    #             beta[k-1,i]  += beta[k,j]*B[j,obs[k]]*A[i,j]\n",
    "    #     beta[k-1] = beta[k-1] * scale_factor[K-1]\n",
    "    return beta\n"
   ]
  },
  {
   "source": [
    "### Smoother"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoother(alpha, beta):\n",
    "    K = len(alpha) #number of observations/time steps\n",
    "    gamma = np.zeros(alpha.shape)\n",
    "\n",
    "    for k in range(K):\n",
    "        alpha_k = alpha[k]\n",
    "        beta_k = beta[k]\n",
    "        num = np.multiply(alpha_k, beta_k)\n",
    "        den = np.sum(num) #sum across all states\n",
    "        #gamma[k] /= scale_factor[k]\n",
    "        gamma[k] = num/np.maximum(1e-300,den) #normalize\n",
    "\n",
    "    # #Equivalent For Loop\n",
    "    # for k in range(K):\n",
    "    #     for i in range(alpha.shape[1]):\n",
    "    #         gamma[k,i] = alpha[k,i] * beta[k,i]\n",
    "    #     gamma[k] = gamma[k]/np.sum(gamma[k])\n",
    "    return gamma"
   ]
  },
  {
   "source": [
    "### Combined Optimal HMM Filter Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM_filter(A, B, pi, obs):\n",
    "    alpha, c = forward_filt(A, B, pi, o)\n",
    "    beta = backward_filt(A, B, o, c)\n",
    "    gamma = smoother(alpha, beta)\n",
    "    return gamma"
   ]
  },
  {
   "source": [
    "### Paired States"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_st(A, B, alpha, beta, obs):\n",
    "    K = len(alpha) #number of observations\n",
    "    S = len(A) #number of states\n",
    "    xi = np.zeros([K,S,S])\n",
    "    for k in range(K):\n",
    "        alpha_k = alpha[k,:,np.newaxis] #add new axis so mmultiplication will work elementwize\n",
    "        beta_k1 = beta[(k+1)%K,:]\n",
    "        o_k1 = obs[(k+1)%K] #next observation (loop around to start for last obs)\n",
    "        B_k1 = B[:,o_k1] #likelihood of next observation for given state\n",
    "\n",
    "        xi[k] = alpha_k * A * B_k1 * beta_k1\n",
    "        # xi[k][xi[k] == 0] = 1e-10\n",
    "        xi[k] = xi[k]/np.maximum(1e-300,np.sum(xi[k])) #normalize\n",
    "\n",
    "        # #Equivalent For Loop\n",
    "        # for i in range(S):\n",
    "        #     for j in range(S):\n",
    "        #         xi[k,i,j] = alpha[k,i]*A[i,j]*B_k1[j]*beta_k1[j]\n",
    "        # xi[k] = xi[k]/np.sum(xi[k]) #normalize\n",
    "    return xi"
   ]
  },
  {
   "source": [
    "## Simulate toy Markov Chain to test filters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_MC(A, B, pi, K):\n",
    "    s = np.zeros(K,dtype=np.uint8) #vector of samples\n",
    "    o = np.zeros(K,dtype=np.uint8) #vector of observations\n",
    "    s[0] = np.random.choice(len(A), 1, p=pi) #sim initial state\n",
    "    for k in range(1,K):\n",
    "        s[k] = np.random.choice(len(A), 1, p=A[s[k-1]]) #sim next state\n",
    "        o[k] = np.random.choice(B.shape[1], 1, p=B[s[k]]) #sim observation\n",
    "    return [s, o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 200\n",
    "\n",
    "#3 possible states, 5 possible observations\n",
    "pi = [0.2, 0.6, 0.2]\n",
    "pi = np.array(pi)\n",
    "\n",
    "# sj   0    1    2  | si\n",
    "A = [[0.8, 0.2, 0.0], #0\n",
    "     [0.1, 0.7, 0.2], #1\n",
    "     [0.3, 0.0, 0.7]] #2\n",
    "A = np.array(A)\n",
    "\n",
    "# ok   0    1    2    3    4  | sk\n",
    "B = [[0.50, 0.30, 0.10, 0.05, 0.05], #0\n",
    "     [0.05, 0.20, 0.50, 0.20, 0.05], #1\n",
    "     [0.05, 0.05, 0.10, 0.30, 0.50]] #s\n",
    "B = np.array(B)\n",
    "\n",
    "s,o = sim_MC(A, B, pi, K)"
   ]
  },
  {
   "source": [
    "### Run filters on toy MC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, c = forward_filt(A, B, pi, o) #forward filter probabilities\n",
    "beta = backward_filt(A, B, o, c) #backward filter probabilities\n",
    "gamma = smoother(alpha, beta) #combined probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for discrete state estimates, take argmax of state probabilities\n",
    "# a_est = np.dot(alpha,range(3)) #forward filter state estimates\n",
    "# b_est = np.dot(beta,range(3)) #backward filter state estimates\n",
    "# g_est = np.dot(gamma,range(3)) #optimal combined state estimates\n",
    "\n",
    "a1_est = np.argmax(alpha_1,axis=1)\n",
    "b1_est = np.argmax(beta_1,axis=1)\n",
    "\n",
    "a_est = np.argmax(alpha,axis=1)\n",
    "b_est = np.argmax(beta,axis=1)\n",
    "g_est = np.argmax(gamma,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(s); #plot true state\n",
    "plt.plot(np.round(o/2)) #plot naive estimate from observations\n",
    "plt.plot(a_est); #plot forward filter estimate\n",
    "# plt.plot(b_est) #plot backward filter estimate\n",
    "# plt.plot(g_est) #plot optimal combined estimate\n",
    "plt.legend(['true state', 'filt']);# 'obs', 'filt'])#, 'backward', 'combined'])\n",
    "plt.title('Comparisson of State Estimates')\n",
    "plt.grid(True,markevery=1)"
   ]
  },
  {
   "source": [
    "### Compare mean-squared errors of each filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_obs = np.mean(((s - np.round(o/2))**2))\n",
    "mse_a_est = np.mean(((s - a_est)**2))\n",
    "mse_b_est = np.mean(((s - b_est)**2))\n",
    "mse_g_est = np.mean(((s - g_est)**2))\n",
    "print('obs mse:\\t', mse_obs)\n",
    "print('fwd mse:\\t', mse_a_est)\n",
    "print('bkwd mse:\\t', mse_b_est)\n",
    "print('opt mse:\\t', mse_g_est)"
   ]
  },
  {
   "source": [
    "### Check that Paired States Function returns correct size and probs sum to 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = paired_st(A, B, alpha, beta, o)\n",
    "xi.shape == (len(alpha),) + A.shape #check that returned shape is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_sums = np.sum(xi,axis=(1,2)) #sum across all i,j for each observation\n",
    "is_ones = np.round(prop_sums,5) == 1 #compare sums to one, rounding to 5 decimals\n",
    "np.all(is_ones) #check that all probabilities sum to one"
   ]
  },
  {
   "source": [
    "## Try running functions on some of our observations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Baum Welsch Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welsch(HMM, obs, N): #(initial guesses,,, observations, number of iterations)\n",
    "    A_est = np.copy(HMM[0])\n",
    "    B_est = np.copy(HMM[1])\n",
    "    pi_est = np.copy(HMM[2])\n",
    "    K = len(obs) #number of observations\n",
    "    S = len(A_est) #number of states\n",
    "    O = B_est.shape[1] #number of observation states\n",
    "    l = np.zeros(N) #likelihoods\n",
    "    for n in range(N):\n",
    "        #E-Step---------------\n",
    "        #HMM Filtering\n",
    "        #---------------------\n",
    "        alpha, c = forward_filt(A_est, B_est, pi_est, obs) #run forward filter\n",
    "        beta = backward_filt(A_est, B_est, obs, c) #run backward filter\n",
    "        gamma = smoother(alpha, beta) #run smoother\n",
    "        xi = paired_st(A_est, B_est, alpha, beta, obs)\n",
    "\n",
    "        l[n] = -np.sum(np.log(c))\n",
    "\n",
    "        # print('Max Vals')\n",
    "        # print('-----------------')\n",
    "        # print('Alpha',np.max(alpha))\n",
    "        # print('Scale',np.max(c))\n",
    "        # print('Beta',np.max(beta))\n",
    "        # print('Gamma',np.max(gamma))\n",
    "        # print('Xi',np.max(xi))\n",
    "        # print()\n",
    "\n",
    "        #M-Step---------------\n",
    "        #update parameter estimates\n",
    "        #---------------------\n",
    "        #update pi_est\n",
    "        pi_est = gamma[0]\n",
    "        \n",
    "        #update A_est\n",
    "        A_num = np.sum(xi,axis=0) #expected num of jumps from i to j\n",
    "        A_den = np.sum(gamma,axis=0)[:,np.newaxis] #total expected visits to i\n",
    "        A_est = A_num/np.maximum(1e-10,A_den)\n",
    "\n",
    "        #update B_est\n",
    "        B_den = np.sum(gamma,axis=0)\n",
    "        #B_den[B_den == 0] = 1e-10\n",
    "        for o in range(B_est.shape[1]): #loop through all possible observations\n",
    "            B_num = np.sum(gamma[obs == o],axis=0) #sum over observations o \n",
    "            B_est[:,o] = B_num/np.maximum(1e-10,B_den)\n",
    "\n",
    "        # #Equivalent For Loop\n",
    "        # for i in range(S): #current state\n",
    "        #     for j in range(S): #next state\n",
    "        #         A_num = np.sum(xi[:,i,j])\n",
    "        #         A_den = np.sum(gamma[:,i])\n",
    "        #         A_est[i,j] = A_num/np.maximum(1e-10,A_den)\n",
    "        #     for o in range(O): #observation state\n",
    "        #         B_num = 0\n",
    "        #         for k in range(K): #observation\n",
    "        #             if(obs[k] == o):\n",
    "        #                 B_num += gamma[k,i]\n",
    "        #         B_den = np.sum(gamma[:,i])\n",
    "        #         B_est[i,o] = B_num/np.maximum(1e-10,B_den)\n",
    "    return [A_est, B_est, pi_est, l]"
   ]
  },
  {
   "source": [
    "### Test Baum Welsch on toy MC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random probability matrix (rows sum to 1)\n",
    "def rand_prob_mat(shape):\n",
    "    p = np.zeros(shape)\n",
    "    nrows = shape[0]\n",
    "    if(len(shape) == 1):\n",
    "        p = np.random.rand(nrows)\n",
    "        p = p/np.sum(p)\n",
    "    elif(len(shape) == 2):\n",
    "        ncols = shape[1]\n",
    "        for i in range(nrows):\n",
    "            p[i,:] = np.random.rand(1,ncols)\n",
    "            p[i,:] = p[i,:]/np.sum(p[i,:])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "s1,o1 = sim_MC(A, B, pi, 300) #simulate a Markov Chain with 200 obs\n",
    "\n",
    "#generate random guesses for A, B, pi\n",
    "A_est = rand_prob_mat(A.shape)\n",
    "B_est = rand_prob_mat(B.shape)\n",
    "pi_est = rand_prob_mat(pi.shape)\n",
    "HMM_est = [A_est, B_est, pi_est]\n",
    "\n",
    "#estimate A, B, pi using baum welsch algorithm\n",
    "A_est, B_est, pi_est, l = baum_welsch(HMM_est, o1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l)"
   ]
  },
  {
   "source": [
    "# Training HMM On IMU Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importing Training Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR_PATH = '../train/'\n",
    "\n",
    "train_files = listdir(TRAIN_DIR_PATH)\n",
    "\n",
    "#CREDIT: Eli gave me the idea to use a dictionary like this rather\n",
    "#        than the messy attempt I had at just using arrays\n",
    "keys = ['wave','inf','eight','circle','beat3','beat4']\n",
    "obs_dict = {k:[] for k in keys}\n",
    "all_obs = []\n",
    "for f in train_files:\n",
    "    obs = np.genfromtxt(fname=TRAIN_DIR_PATH + f, usecols=[1,2,3,4,5,6]) #read observations from file\n",
    "    all_obs.append(obs)\n",
    "    if('wave' in f):\n",
    "        obs_dict['wave'].append(obs)\n",
    "    elif('inf' in f):\n",
    "        obs_dict['inf'].append(obs)\n",
    "    elif('eight' in f):\n",
    "        obs_dict['eight'].append(obs)\n",
    "    elif('circle' in f):\n",
    "        obs_dict['circle'].append(obs)\n",
    "    elif('beat3' in f):\n",
    "        obs_dict['beat3'].append(obs)\n",
    "    elif('beat4' in f):\n",
    "        obs_dict['beat4'].append(obs)\n",
    "all_obs = np.concatenate(all_obs)"
   ]
  },
  {
   "source": [
    "### Classify/Cluster Observations using KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKMeans:\n",
    "    def __init__(self, n_clusters, cutoff=0.05, max_iter = 500):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroids = np.zeros(n_clusters)\n",
    "        self.cutoff = cutoff\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _sos(self, arr): #sum of squares\n",
    "        return np.sum(np.power(arr,2),axis=tuple(range(1,arr.ndim)))\n",
    "\n",
    "    #CREDIT: Eli told me about this trick to calculate distances on the entire arrays\n",
    "    #        to avoid looping though every element\n",
    "    def _dist_arrs(self, X1, X2): #l2 norm between vectors in 2 arrays\n",
    "        x1_sos = np.expand_dims(self._sos(X1), axis=1)\n",
    "        x2_sos = self._sos(X2)\n",
    "        dot_sos = -2*X1.dot(X2.T)\n",
    "        sos = np.round(x1_sos+x2_sos+dot_sos,10) #round to 10 decimals to avoid negative due to machine error\n",
    "        return np.sqrt(sos)\n",
    "\n",
    "    def _dist(self,x1,x2): #l2 norm between 2 vectors\n",
    "        sq_err = np.power((x1 - x2),2)\n",
    "        sm = np.sum(sq_err)\n",
    "        return np.sqrt(sm)\n",
    "\n",
    "    def _calc_centroids(self, X, labels): #calculate new centroids as mean position of all x with same label\n",
    "        n_cents = np.array([np.mean(X[labels == i],axis=0) for i in range(self.n_clusters)])\n",
    "        n_cents[np.isnan(n_cents)] = self.centroids[np.isnan(n_cents)] #any centroids with no members remain stationary\n",
    "        return np.array([np.mean(X[labels == i],axis=0) for i in range(self.n_clusters)])\n",
    "    \n",
    "    def _nearest_centroid_arr(self, X): #calculate nearest centroid for each element in array\n",
    "        dist = self._dist_arrs(X,self.centroids)\n",
    "        labels = np.argmin(dist,axis=1) #label with index of nearest centroid\n",
    "        return labels\n",
    "\n",
    "    def _nearest_centroid(self, x):\n",
    "        min_dist = 1e100\n",
    "        label = 0\n",
    "        for i,cent in enumerate(self.centroids):\n",
    "            d = self._dist(x,cent)\n",
    "            if(d < min_dist):\n",
    "                label = i\n",
    "                min_dist = d\n",
    "        # dist = [self._dist(x,cent) for cent in self.centroids] #calculate distances to centroids\n",
    "        # label = np.argmin(dist) #label with index of closest centroid\n",
    "        return label\n",
    "\n",
    "    def fit(self,X):\n",
    "        X = np.array(X)\n",
    "        cent_idx = np.random.choice(range(len(X)), self.n_clusters, replace=False) #indices of x's that will be used as initial centroids\n",
    "        self.centroids = np.array([X[i] for i in cent_idx])\n",
    "        xlabels = self._nearest_centroid_arr(X) #label each sample with nearest centroid\n",
    "        training = True\n",
    "        cnt = 0\n",
    "        while training:\n",
    "            cnt = cnt + 1\n",
    "            n_centroids = self._calc_centroids(X, xlabels) #new centroids at mean of labeled samples\n",
    "            diff = np.max([self._dist(n_centroids[i], centroid) for i,centroid in enumerate(self.centroids)]) #distance centroids moved\n",
    "            self.centroids = n_centroids #update centroids\n",
    "            xlabels = self._nearest_centroid_arr(X) #label each sample x by nearest centroid\n",
    "            training = diff > self.cutoff #exit loop if no centroid moves by more than cutoff\n",
    "            if(cnt > self.max_iter):\n",
    "                print('Maximum Iterations',self.max_iter,'reached before cunvergence')\n",
    "                training = False\n",
    "        return self\n",
    "\n",
    "    def predict(self,x):\n",
    "        if(x.ndim > 1):\n",
    "            return self._nearest_centroid_arr(x)\n",
    "        else:\n",
    "            return self._nearest_centroid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = [[1,2],\n",
    "     [3,1],\n",
    "     [0,0],\n",
    "     [0,3],\n",
    "     [7,9],\n",
    "     [10,6],\n",
    "     [8,8]]\n",
    "X = np.array(X)\n",
    "\n",
    "kt = MyKMeans(n_clusters = 2)\n",
    "kt.fit(X)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1]);\n",
    "plt.scatter(kt.centroids[:,0], kt.centroids[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cent = np.array([[-5,-3,0], [0,0,0], [-5,-5,-5]])\n",
    "\n",
    "x1 = true_cent[0] + 5*np.random.rand(500,3)\n",
    "x2 = true_cent[1] + 5*np.random.rand(500,3)\n",
    "x3 = true_cent[2] + 5*np.random.rand(500,3)\n",
    "x = np.concatenate([x1,x2,x3])\n",
    "\n",
    "kt = MyKMeans(n_clusters=3).fit(x)\n",
    "plt.scatter(x[:,0],x[:,1]);\n",
    "plt.scatter(kt.centroids[:,0],kt.centroids[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates = 20 #number of possible states\n",
    "nobs = 100 #number of possible observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clustering wave ...\n",
      "clustering inf ...\n",
      "clustering eight ...\n",
      "clustering circle ...\n",
      "clustering beat3 ...\n",
      "clustering beat4 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kmeans = MyKMeans(n_clusters=nobs).fit(all_obs) #train k-means on all observations\n",
    "\n",
    "c_obs_dict = {}\n",
    "for k in keys:\n",
    "    print('clustering',k,'...')\n",
    "    c_obs_dict[k] = [kmeans.predict(obs) for obs in obs_dict[k]] #classified/clustered observations\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "#Pickle kmeans\n",
    "fname = 'kmeans.pkl'\n",
    "f = open(fname,'wb')\n",
    "pkl.dump(kmeans,f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-5edb75e90335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'kmeans.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "#Read pickled kmeans file to avoid retraining models\n",
    "fname = 'kmeans.pkl'\n",
    "f = open(fname,'rb')\n",
    "kmeans = pkl.load(f)\n",
    "f.close\n",
    "\n",
    "c_obs_dict = {}\n",
    "for k in keys:\n",
    "    c_obs_dict[k] = [kmeans.predict(obs) for obs in obs_dict[k]] #classified/clustered observations"
   ]
  },
  {
   "source": [
    "### Train HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training wave ...done\n",
      "training inf ...done\n",
      "training eight ...done\n",
      "training circle ...done\n",
      "training beat3 ...done\n",
      "training beat4 ...done\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "N = 50 #number of iterations for Baum-Welsch\n",
    "\n",
    "A_rand = rand_prob_mat((nstates,nstates))\n",
    "B_rand = rand_prob_mat((nstates,nobs))\n",
    "pi_rand = rand_prob_mat((nstates,))\n",
    "\n",
    "likelihoods = {}\n",
    "HMMs = {}\n",
    "for k in keys:\n",
    "    print(\"training\",k,'...',end='')\n",
    "    HMMs[k] = [A_rand, B_rand, pi_rand] #randomly initialize HMM\n",
    "    big_obs_vect = np.concatenate(c_obs_dict[k])\n",
    "    bw = baum_welsch(HMMs[k], big_obs_vect, N)\n",
    "    HMMs[k] = bw[0:3]\n",
    "    likelihoods[k] = bw[3]\n",
    "    # for c_obs in c_obs_dict[k]: #loop through observation files\n",
    "    #     HMMs[k] = baum_welsch(HMMs[k], c_obs, N) #train HMM\n",
    "    #     print('.',end='')\n",
    "    print('done')\n",
    "    for param in HMMs[k]:\n",
    "        assert not np.any(np.isnan(param)) #error if there are any nans\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_rand = rand_prob_mat((nstates,nstates))\n",
    "B_rand = rand_prob_mat((nstates,nobs))\n",
    "pi_rand = rand_prob_mat((nstates,))\n",
    "HMM1 = [A_rand, B_rand, pi_rand]\n",
    "_, _, _, l1 = baum_welsch(HMM1, np.concatenate(c_obs_dict['eight']), N)\n",
    "\n",
    "plt.plot(l1)"
   ]
  },
  {
   "source": [
    "### Pickle HMMs for later use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store trained HMMs in pkl because they take forever to train\n",
    "fname = 'HMMs.pkl'\n",
    "f = open(fname,'wb')\n",
    "pkl.dump(HMMs,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pickled HMM file to avoid retraining models\n",
    "fname = 'HMMs.pkl'\n",
    "f = open(fname,'rb')\n",
    "HMMs = pkl.load(f)\n",
    "f.close"
   ]
  },
  {
   "source": [
    "## Predict Gestures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_mod_lik(HMM, obs): #likelihood that given observations came from given HMM\n",
    "    K = len(obs) #number of observations\n",
    "    A, B, pi = HMM\n",
    "    _, c = forward_filt(A, B, pi, obs) #forward filter\n",
    "    lik = -np.sum(np.log(c)) #calculate likelihood of seeing these obs under this HMM\n",
    "    return lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gesture(HMMs, obs):\n",
    "    keys = np.array(list(HMMs.keys()))\n",
    "    l = np.zeros(len(keys))\n",
    "    max_l = 0\n",
    "    pred = 'none'\n",
    "    c_obs = kmeans.predict(obs)\n",
    "    for i,k in enumerate(keys):\n",
    "        #c_obs = kmeans[k].predict(obs)\n",
    "        l[i] = obs_mod_lik(HMMs[k], c_obs)\n",
    "        if(k == 'wave'):\n",
    "            l[i] *= 0.6 #likelihood of wave artificially increased because of how\n",
    "                        #it relates to likelihood of inf\n",
    "    # arrange likelihoods and keys in descending order\n",
    "    sort_ind = l.argsort()\n",
    "    sorted_l = l[sort_ind[::-1]]\n",
    "    sorted_keys = keys[sort_ind[::-1]]\n",
    "    return [sorted_keys,sorted_l] #returns gestures in order of decreasing likelihood and associated likelihoods"
   ]
  },
  {
   "source": [
    "# Predict On Test Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'listdir' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-22df8b747439>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mTEST_DIR_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../old_test/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_DIR_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTEST_DIR_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#read observations from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'listdir' is not defined"
     ]
    }
   ],
   "source": [
    "TEST_DIR_PATH = '../old_test/'\n",
    "test_files = listdir(TEST_DIR_PATH)\n",
    "\n",
    "for f in test_files:\n",
    "    obs = np.genfromtxt(fname=TEST_DIR_PATH + f, usecols=[1,2,3,4,5,6]) #read observations from file\n",
    "    pred,ls = classify_gesture(HMMs, obs)\n",
    "    print('----',f,'----')\n",
    "    for i in range(3):\n",
    "        print(i+1, '. ', pred[i], '\\t', ls[i], sep='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}