{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "14e8a30fe7d1f8eb3ea56e6d13e3789d1eba5d72906091ff4aff2300b33fa113"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gesture Recognition with Hidden Markov Models\n",
    "## HMM Filters\n",
    "### Forward Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import pickle as pkl\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_filt(A, B, pi, obs):\n",
    "    #q_ki = P(s_k = i, y_1:k)\n",
    "    alpha = np.zeros((len(obs),len(A))) #n_obs x n_states\n",
    "    scale_factor = np.zeros(len(obs))\n",
    "\n",
    "    #initialization\n",
    "    o_k = obs[0] #scalar observation at time step k\n",
    "    B_k = np.diag(B[:,o_k]) #4x4 diagonal matrix\n",
    "    alpha[0] = np.dot(B_k,pi) #4x1 vector\n",
    "    alpha[0][alpha[0] == 0] = 1e-300\n",
    "    #scale_factor[0] = 1/np.sum(alpha[0])\n",
    "    #alpha[0] = alpha[0] * scale_factor[0]#/np.sum(alpha[0])\n",
    "    alpha[0] = alpha[0]/np.sum(alpha[0])\n",
    "    for k in range(1,len(obs)):\n",
    "        o_k = obs[k]\n",
    "        B_k = np.diag(B[:,o_k])\n",
    "        alpha[k] = np.dot(np.dot(B_k,A),alpha[k-1]) #alpha[k] = B_k * A * alpha[k-1]\n",
    "        alpha[k][alpha[k] == 0] = 1e-300\n",
    "        # scale_factor[k] = 1/np.sum(alpha[k])\n",
    "        # alpha[k] = alpha[k] * scale_factor[k]\n",
    "        alpha[k] = alpha[k]/np.sum(alpha[k]) #normalize\n",
    "    return [alpha, scale_factor]"
   ]
  },
  {
   "source": [
    "### Backward Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_filt(A, B, obs, scale_factor):\n",
    "    K = len(obs)\n",
    "    beta = np.zeros((K,len(A)))\n",
    "    #initialization\n",
    "    beta[K-1] = np.ones(len(A))\n",
    "    beta[K-1] = beta[K-1]/len(A)\n",
    "    #beta[K-1] = beta[K-1] * scale_factor[K-1]\n",
    "    for k in range(K-1,0,-1):\n",
    "        o_k = obs[k]\n",
    "        B_k = np.diag(B[:,o_k]) #diagonalize for elementwise multiplication w/ dot prod\n",
    "        beta[k-1] = np.dot(np.dot(A,B_k),beta[k]) #beta[k-1] = A * B_k * beta[k]\n",
    "        beta[k-1][beta[k-1] == 0] = 1e-300\n",
    "        beta[k-1] = beta[k-1]/np.sum(beta[k-1])\n",
    "        #beta[k-1] = beta[k-1] * scale_factor[k-1]\n",
    "    return beta\n"
   ]
  },
  {
   "source": [
    "### Smoother"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoother(alpha, beta):\n",
    "    K = len(alpha) #number of observations/time steps\n",
    "    gamma = np.zeros(alpha.shape)\n",
    "    for k in range(K):\n",
    "        alpha_k = alpha[k]\n",
    "        beta_k = beta[k]\n",
    "        num = np.multiply(alpha_k, beta_k)\n",
    "        den = np.sum(num) #sum across all states\n",
    "        gamma[k] = num/den #normalize\n",
    "    return gamma"
   ]
  },
  {
   "source": [
    "### Paired States"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_st(A, B, alpha, beta, obs):\n",
    "    K = len(alpha) #number of observations\n",
    "    X = alpha.shape[1] #number of states\n",
    "    xi = np.zeros([K-1,X,X])\n",
    "    for k in range(K-1):\n",
    "        alpha_k = alpha[k,:,np.newaxis]\n",
    "        beta_k1 = beta[k+1,:]\n",
    "\n",
    "        o_k1 = obs[k+1]\n",
    "        B_k1 = B[:,o_k1]\n",
    "\n",
    "        xi[k] = alpha_k * A * B_k1 * beta_k1\n",
    "        xi[k][xi[k] == 0] = 1e-300\n",
    "        xi[k] = xi[k]/np.sum(xi[k]) #normalize\n",
    "    return xi"
   ]
  },
  {
   "source": [
    "## Baum-Welsch Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welsch(HMM, obs, N): #(initial guesses,,, observations, number of iterations)\n",
    "    A_est = np.copy(HMM[0])\n",
    "    B_est = np.copy(HMM[1])\n",
    "    pi_est = np.copy(HMM[2])\n",
    "    for n in range(N):\n",
    "        #E-Step---------------\n",
    "        #HMM Filtering\n",
    "        #---------------------\n",
    "        alpha, c = forward_filt(A_est, B_est, pi_est, obs) #run forward filter\n",
    "        beta = backward_filt(A_est, B_est, obs, c) #run backward filter\n",
    "        gamma = smoother(alpha, beta) #run smoother\n",
    "        xi = paired_st(A_est, B_est, alpha, beta, obs)\n",
    "\n",
    "        #M-Step---------------\n",
    "        #update parameter estimates\n",
    "        #---------------------\n",
    "        #update pi_est\n",
    "        pi_est = gamma[0]\n",
    "\n",
    "        #update A_est\n",
    "        A_num = np.sum(xi,axis=0) #expected num of jumps from i to j\n",
    "        A_den = np.sum(gamma[0:-1],axis=0)[:,np.newaxis] #total expected visits to i\n",
    "        A_den[A_den == 0] = 1e-300\n",
    "        A_est = A_num/A_den\n",
    "\n",
    "        #update B_est\n",
    "        B_den = np.sum(gamma,axis=0)\n",
    "        B_den[B_den == 0] = 1e-300\n",
    "        for o in range(B_est.shape[1]): #loop through all possible observations\n",
    "            B_num = np.sum(gamma[obs == o],axis=0) #sum over observations o \n",
    "            B_est[:,o] = B_num/B_den\n",
    "\n",
    "    return [A_est, B_est, pi_est]"
   ]
  },
  {
   "source": [
    "## K-Means Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKMeans:\n",
    "    def __init__(self, n_clusters, cutoff=0.05, max_iter = 500):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroids = np.zeros(n_clusters)\n",
    "        self.cutoff = cutoff\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _sos(self, arr): #sum of squares\n",
    "        return np.sum(np.power(arr,2),axis=tuple(range(1,arr.ndim)))\n",
    "\n",
    "    #CREDIT: Eli told me about this trick to calculate distances on the entire arrays\n",
    "    #        to avoid looping though every element\n",
    "    def _dist_arrs(self, X1, X2): #l2 norm between vectors in 2 arrays\n",
    "        x1_sos = np.expand_dims(self._sos(X1), axis=1)\n",
    "        x2_sos = self._sos(X2)\n",
    "        dot_sos = -2*X1.dot(X2.T)\n",
    "        sos = np.round(x1_sos+x2_sos+dot_sos,10) #round to 10 decimals to avoid negative due to machine error\n",
    "        return np.sqrt(sos)\n",
    "\n",
    "    def _dist(self,x1,x2): #l2 norm between 2 vectors\n",
    "        sq_err = np.power((x1 - x2),2)\n",
    "        sm = np.sum(sq_err)\n",
    "        return np.sqrt(sm)\n",
    "\n",
    "    def _calc_centroids(self, X, labels): #calculate new centroids as mean position of all x with same label\n",
    "        n_cents = np.array([np.mean(X[labels == i],axis=0) for i in range(self.n_clusters)])\n",
    "        n_cents[np.isnan(n_cents)] = self.centroids[np.isnan(n_cents)] #any centroids with no members remain stationary\n",
    "        return np.array([np.mean(X[labels == i],axis=0) for i in range(self.n_clusters)])\n",
    "    \n",
    "    def _nearest_centroid_arr(self, X): #calculate nearest centroid for each element in array\n",
    "        dist = self._dist_arrs(X,self.centroids)\n",
    "        labels = np.argmin(dist,axis=1) #label with index of nearest centroid\n",
    "        return labels\n",
    "\n",
    "    def _nearest_centroid(self, x):\n",
    "        min_dist = 1e100\n",
    "        label = 0\n",
    "        for i,cent in enumerate(self.centroids):\n",
    "            d = self._dist(x,cent)\n",
    "            if(d < min_dist):\n",
    "                label = i\n",
    "                min_dist = d\n",
    "        # dist = [self._dist(x,cent) for cent in self.centroids] #calculate distances to centroids\n",
    "        # label = np.argmin(dist) #label with index of closest centroid\n",
    "        return label\n",
    "\n",
    "    def fit(self,X):\n",
    "        X = np.array(X)\n",
    "        cent_idx = np.random.choice(range(len(X)), self.n_clusters, replace=False) #indices of x's that will be used as initial centroids\n",
    "        self.centroids = np.array([X[i] for i in cent_idx])\n",
    "        xlabels = self._nearest_centroid_arr(X) #label each sample with nearest centroid\n",
    "        training = True\n",
    "        cnt = 0\n",
    "        while training:\n",
    "            cnt = cnt + 1\n",
    "            n_centroids = self._calc_centroids(X, xlabels) #new centroids at mean of labeled samples\n",
    "            diff = np.max([self._dist(n_centroids[i], centroid) for i,centroid in enumerate(self.centroids)]) #distance centroids moved\n",
    "            self.centroids = n_centroids #update centroids\n",
    "            xlabels = self._nearest_centroid_arr(X) #label each sample x by nearest centroid\n",
    "            training = diff > self.cutoff #exit loop if no centroid moves by more than cutoff\n",
    "            if(cnt > self.max_iter):\n",
    "                print('Maximum Iterations',self.max_iter,'reached before cunvergence')\n",
    "                training = False\n",
    "        return self\n",
    "\n",
    "    def predict(self,x):\n",
    "        if(x.ndim > 1):\n",
    "            return self._nearest_centroid_arr(x)\n",
    "        else:\n",
    "            return self._nearest_centroid(x)"
   ]
  },
  {
   "source": [
    "## Training Models\n",
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR_PATH = '../train/'\n",
    "\n",
    "train_files = listdir(TRAIN_DIR_PATH)\n",
    "\n",
    "#CREDIT: Eli gave me the idea to use a dictionary like this rather\n",
    "#        than the messy attempt I had at just using arrays\n",
    "keys = ['wave','inf','eight','circle','beat3','beat4']\n",
    "obs_dict = {k:[] for k in keys}\n",
    "for f in train_files:\n",
    "    obs = np.genfromtxt(fname=TRAIN_DIR_PATH + f, usecols=[1,2,3,4,5,6]) #read observations from file\n",
    "    if('wave' in f):\n",
    "        obs_dict['wave'].append(obs)\n",
    "    elif('inf' in f):\n",
    "        obs_dict['inf'].append(obs)\n",
    "    elif('eight' in f):\n",
    "        obs_dict['eight'].append(obs)\n",
    "    elif('circle' in f):\n",
    "        obs_dict['circle'].append(obs)\n",
    "    elif('beat3' in f):\n",
    "        obs_dict['beat3'].append(obs)\n",
    "    elif('beat4' in f):\n",
    "        obs_dict['beat4'].append(obs)"
   ]
  },
  {
   "source": [
    "### Cluster Observations using KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates = 20 #number of possible states\n",
    "nobs = 100 #number of possible observations\n",
    "\n",
    "kmeans = {}\n",
    "c_obs_dict = {}\n",
    "for k in keys:\n",
    "    kmeans[k] = MyKMeans(n_clusters=nobs,cutoff=0.1).fit(np.concatenate(obs_dict[k])) #train k-means\n",
    "    c_obs_dict[k] = [kmeans[k].predict(obs) for obs in obs_dict[k]] #classified/clustered observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates = 20 #number of possible states\n",
    "nobs = 100 #number of possible observations\n",
    "\n",
    "kmeans = {}\n",
    "c_obs_dict = {}\n",
    "for k in keys:\n",
    "    kmeans[k] = KMeans(n_clusters=nobs, random_state=0).fit(np.concatenate(obs_dict[k])) #train k-means\n",
    "    c_obs_dict[k] = [kmeans[k].predict(obs) for obs in obs_dict[k]] #classified/clustered observations"
   ]
  },
  {
   "source": [
    "### Pickle KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store trained kmeans in pkl because they take forever to train\n",
    "fname = 'kmeans.pkl'\n",
    "f = open(fname,'wb')\n",
    "pkl.dump(kmeans,f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "### Load Pickled KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#Read pickled kmeans file to avoid retraining models\n",
    "fname = 'kmeans.pkl'\n",
    "f = open(fname,'rb')\n",
    "kmeans = pkl.load(f)\n",
    "f.close"
   ]
  },
  {
   "source": [
    "### Train HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random probability matrix (rows sum to 1)\n",
    "def rand_prob_mat(shape):\n",
    "    p = np.zeros(shape)\n",
    "    nrows = shape[0]\n",
    "    if(len(shape) == 1):\n",
    "        p = np.random.rand(nrows)\n",
    "        p = p/np.sum(p)\n",
    "    elif(len(shape) == 2):\n",
    "        ncols = shape[1]\n",
    "        for i in range(nrows):\n",
    "            p[i,:] = np.random.rand(1,ncols)\n",
    "            p[i,:] = p[i,:]/np.sum(p[i,:])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training wave.....done\n",
      "training inf.....done\n",
      "training eight.....done\n",
      "training circle.....done\n",
      "training beat3.....done\n",
      "training beat4.....done\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "N = 50 #number of iterations for Baum-Welsch\n",
    "\n",
    "A_rand = rand_prob_mat((nstates,nstates))\n",
    "B_rand = rand_prob_mat((nstates,nobs))\n",
    "pi_rand = rand_prob_mat((nstates,))\n",
    "\n",
    "HMMs = {}\n",
    "for k in keys:\n",
    "    print(\"training\",k,end='')\n",
    "    HMMs[k] = [A_rand, B_rand, pi_rand] #randomly initialize HMM\n",
    "    for c_obs in c_obs_dict[k]: #loop through observation files\n",
    "        HMMs[k] = baum_welsch(HMMs[k], c_obs, N) #train HMM\n",
    "        print('.',end='')\n",
    "    print('done')\n",
    "    for param in HMMs[k]:\n",
    "        assert not np.any(np.isnan(param)) #error if there are any nans\n",
    "print('Done!')"
   ]
  },
  {
   "source": [
    "### Pickle Trained HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store trained HMMs in pkl because they take forever to train\n",
    "fname = 'HMMs.pkl'\n",
    "f = open(fname,'wb')\n",
    "pkl.dump(HMMs,f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "### Load Pickled HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "#Read pickled HMM file to avoid retraining models\n",
    "fname = 'HMMs.pkl'\n",
    "f = open(fname,'rb')\n",
    "HMMs = pkl.load(f)\n",
    "f.close"
   ]
  },
  {
   "source": [
    "## Predict Gestures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_mod_lik(HMM, obs): #likelihood that given observations came from given HMM\n",
    "    A, B, pi = HMM\n",
    "    alpha, c = forward_filt(A, B, pi, obs) #forward filter\n",
    "    beta = backward_filt(A, B, obs, c) #backward filter\n",
    "    lik = np.sum(beta[0] * pi) #calculate probability of seeing these obs under this HMM\n",
    "    return lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gesture(HMMs, obs):\n",
    "    keys = np.array(list(HMMs.keys()))\n",
    "    l = np.zeros(len(keys))\n",
    "    max_l = 0\n",
    "    pred = 'none'\n",
    "    for i,k in enumerate(keys):\n",
    "        c_obs = kmeans[k].predict(obs)\n",
    "        l[i] = obs_mod_lik(HMMs[k], c_obs)\n",
    "    # arrange likelihoods and keys in descending order\n",
    "    sort_ind = l.argsort()\n",
    "    sorted_l = l[sort_ind[::-1]]\n",
    "    sorted_keys = keys[sort_ind[::-1]]\n",
    "    return [sorted_keys,sorted_l] #returns gestures in order of decreasing likelihood and associated likelihoods"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- beat3_31.txt ----\n",
      "1. beat4\t0.3132756053324105\n",
      "2. wave\t0.05\n",
      "3. circle\t0.04999999999999999\n",
      "\n",
      "---- beat4_31.txt ----\n",
      "1. wave\t0.05\n",
      "2. beat4\t0.04999999999999999\n",
      "3. beat3\t0.04999999999999999\n",
      "\n",
      "---- circle31.txt ----\n",
      "1. circle\t0.04999999999999999\n",
      "2. eight\t0.04999999999999999\n",
      "3. inf\t0.04999999999999999\n",
      "\n",
      "---- eight31.txt ----\n",
      "1. wave\t0.05\n",
      "2. eight\t0.04999999999999999\n",
      "3. inf\t0.04999999999999999\n",
      "\n",
      "---- inf31.txt ----\n",
      "1. wave\t0.05\n",
      "2. eight\t0.04999999999999999\n",
      "3. inf\t0.04999999999999999\n",
      "\n",
      "---- wave31.txt ----\n",
      "1. beat4\t0.3132756053324105\n",
      "2. eight\t0.04999999999999999\n",
      "3. inf\t0.04999999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR_PATH = '../old_test/'\n",
    "test_files = listdir(TEST_DIR_PATH)\n",
    "\n",
    "for f in test_files:\n",
    "    obs = np.genfromtxt(fname=TEST_DIR_PATH + f, usecols=[1,2,3,4,5,6]) #read observations from file\n",
    "    pred,ls = classify_gesture(HMMs, obs)\n",
    "    print('----',f,'----')\n",
    "    for i in range(3):\n",
    "        print(i+1, '. ', pred[i], '\\t', ls[i], sep='')\n",
    "    print()"
   ]
  }
 ]
}