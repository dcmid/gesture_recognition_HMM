{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "14e8a30fe7d1f8eb3ea56e6d13e3789d1eba5d72906091ff4aff2300b33fa113"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gesture Recognition with Hidden Markov Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import pickle as pkl"
   ]
  },
  {
   "source": [
    "## HMM Filters\n",
    "### Forward Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_filt(A, B, pi, obs):\n",
    "    K = len(obs) #number of observations\n",
    "    S = len(A) #number of states\n",
    "    #q_ki = P(s_k = i, y_1:k)\n",
    "    alpha = np.zeros((K,S))\n",
    "    scale_factor = np.zeros(K)\n",
    "\n",
    "    #initialization\n",
    "    alpha[0] = np.multiply(B[:,obs[0]],pi) #4x1 vector  \n",
    "    scale_factor[0] = 1/np.maximum(1e-300,np.sum(alpha[0]))\n",
    "    alpha[0] *= scale_factor[0] #normalize\n",
    "\n",
    "    for k in range(1,K):\n",
    "        B_k = np.diag(B[:,obs[k]])\n",
    "        alpha[k] = np.dot(np.dot(B_k,A.T),alpha[k-1]) #P(j,o..) = P(i,o..)P(j|i)P(o|j)\n",
    "        scale_factor[k] = 1/np.maximum(1e-300,np.sum(alpha[k]))\n",
    "        alpha[k] = alpha[k] * scale_factor[k] #normalize\n",
    "\n",
    "    # #Equivalent For Loop\n",
    "    # for k in range(1,len(obs)):\n",
    "    #     for j in range(len(A)):\n",
    "    #         for i in range(len(A)):\n",
    "    #             alpha[k,j] += alpha[k-1,i]*A[i,j]*B[j,obs[k]]\n",
    "    #     scale_factor[k] = 1/np.sum(alpha[k])\n",
    "    #     alpha[k] = alpha[k] * scale_factor[k]\n",
    "\n",
    "    return [alpha, scale_factor]"
   ]
  },
  {
   "source": [
    "### Backward Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_filt(A, B, obs, scale_factor):\n",
    "    nstates = len(A)\n",
    "    K = len(obs)\n",
    "    beta = np.zeros((K,nstates))\n",
    "    #initialization\n",
    "    beta[K-1] = np.ones(nstates)\n",
    "    #beta[K-1][beta[K-1] == 0] = 1e-10\n",
    "    #beta[K-1] = beta[K-1]/len(A)\n",
    "    beta[K-1] *= scale_factor[K-1]\n",
    "    for k in range(K-1,0,-1):\n",
    "        B_k = np.diag(B[:,obs[k]]) #diagonalize for elementwise multiplication w/ dot prod\n",
    "        beta[k-1] = np.dot(np.dot(A,B_k),beta[k]) #beta[k-1] = A * B_k * beta[k]\n",
    "        beta[k-1] *= scale_factor[k-1]\n",
    "        \n",
    "    # #Equivalent For Loop\n",
    "    # for k in range(K-1,0,-1):\n",
    "    #     for i in range(nstates):\n",
    "    #         for j in range(nstates):\n",
    "    #             beta[k-1,i]  += beta[k,j]*B[j,obs[k]]*A[i,j]\n",
    "    #     beta[k-1] = beta[k-1] * scale_factor[K-k]\n",
    "    return beta\n"
   ]
  },
  {
   "source": [
    "### Smoother"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoother(alpha, beta):\n",
    "    K = len(alpha) #number of observations/time steps\n",
    "    gamma = np.zeros(alpha.shape)\n",
    "\n",
    "    for k in range(K):\n",
    "        alpha_k = alpha[k]\n",
    "        beta_k = beta[k]\n",
    "        num = np.multiply(alpha_k, beta_k)\n",
    "        den = np.sum(num) #sum across all states\n",
    "        gamma[k] = num/np.maximum(1e-300,den) #normalize\n",
    "\n",
    "    # #Equivalent For Loop\n",
    "    # for k in range(K):\n",
    "    #     for i in range(alpha.shape[1]):\n",
    "    #         gamma[k,i] = alpha[k,i] * beta[k,i]\n",
    "    #     gamma[k] = gamma[k]/np.sum(gamma[k])\n",
    "    return gamma"
   ]
  },
  {
   "source": [
    "### Paired States"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_st(A, B, alpha, beta, obs):\n",
    "    K = len(alpha) #number of observations\n",
    "    S = len(A) #number of states\n",
    "    xi = np.zeros([K,S,S])\n",
    "    for k in range(K):\n",
    "        alpha_k = alpha[k,:,np.newaxis] #add new axis so mmultiplication will work elementwize\n",
    "        beta_k1 = beta[(k+1)%K,:]\n",
    "        o_k1 = obs[(k+1)%K] #next observation (loop around to start for last obs)\n",
    "        B_k1 = B[:,o_k1] #likelihood of next observation for given state\n",
    "\n",
    "        xi[k] = alpha_k * A * B_k1 * beta_k1\n",
    "        xi[k] = xi[k]/np.maximum(1e-300,np.sum(xi[k])) #normalize\n",
    "\n",
    "        # #Equivalent For Loop\n",
    "        # for i in range(S):\n",
    "        #     for j in range(S):\n",
    "        #         xi[k,i,j] = alpha[k,i]*A[i,j]*B_k1[j]*beta_k1[j]\n",
    "        # xi[k] = xi[k]/np.sum(xi[k]) #normalize\n",
    "    return xi"
   ]
  },
  {
   "source": [
    "## Baum-Welsch Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welsch(HMM, obs, N): #(initial guesses,,, observations, number of iterations)\n",
    "    A_est = np.copy(HMM[0])\n",
    "    B_est = np.copy(HMM[1])\n",
    "    pi_est = np.copy(HMM[2])\n",
    "    K = len(obs) #number of observations\n",
    "    S = len(A_est) #number of states\n",
    "    O = B_est.shape[1] #number of observation states\n",
    "    l = np.zeros(N) #likelihoods\n",
    "    for n in range(N):\n",
    "        #E-Step---------------\n",
    "        #HMM Filtering\n",
    "        #---------------------\n",
    "        alpha, c = forward_filt(A_est, B_est, pi_est, obs) #run forward filter\n",
    "        beta = backward_filt(A_est, B_est, obs, c) #run backward filter\n",
    "        gamma = smoother(alpha, beta) #run smoother\n",
    "        xi = paired_st(A_est, B_est, alpha, beta, obs)\n",
    "\n",
    "        l[n] = -np.sum(np.log(c)) #log likelihood of obs on HMM\n",
    "\n",
    "        #M-Step---------------\n",
    "        #update parameter estimates\n",
    "        #---------------------\n",
    "        #update pi_est\n",
    "        pi_est = gamma[0]\n",
    "        \n",
    "        #update A_est\n",
    "        A_num = np.sum(xi,axis=0) #expected num of jumps from i to j\n",
    "        A_den = np.sum(gamma,axis=0)[:,np.newaxis] #total expected visits to i\n",
    "        A_est = A_num/np.maximum(1e-10,A_den)\n",
    "\n",
    "        #update B_est\n",
    "        B_den = np.sum(gamma,axis=0)\n",
    "        for o in range(B_est.shape[1]): #loop through all possible observations\n",
    "            B_num = np.sum(gamma[obs == o],axis=0) #sum over observations o \n",
    "            B_est[:,o] = B_num/np.maximum(1e-10,B_den)\n",
    "\n",
    "        # #Equivalent For Loop\n",
    "        # for i in range(S): #current state\n",
    "        #     for j in range(S): #next state\n",
    "        #         A_num = np.sum(xi[:,i,j])\n",
    "        #         A_den = np.sum(gamma[:,i])\n",
    "        #         A_est[i,j] = A_num/np.maximum(1e-10,A_den)\n",
    "        #     for o in range(O): #observation state\n",
    "        #         B_num = 0\n",
    "        #         for k in range(K): #observation\n",
    "        #             if(obs[k] == o):\n",
    "        #                 B_num += gamma[k,i]\n",
    "        #         B_den = np.sum(gamma[:,i])\n",
    "        #         B_est[i,o] = B_num/np.maximum(1e-10,B_den)\n",
    "    return [A_est, B_est, pi_est, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random probability matrix (rows sum to 1)\n",
    "def rand_prob_mat(shape):\n",
    "    p = np.zeros(shape)\n",
    "    nrows = shape[0]\n",
    "    if(len(shape) == 1):\n",
    "        p = np.random.rand(nrows)\n",
    "        p = p/np.sum(p)\n",
    "    elif(len(shape) == 2):\n",
    "        ncols = shape[1]\n",
    "        for i in range(nrows):\n",
    "            p[i,:] = np.random.rand(1,ncols)\n",
    "            p[i,:] = p[i,:]/np.sum(p[i,:])\n",
    "    return p"
   ]
  },
  {
   "source": [
    "## K-Means Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKMeans:\n",
    "    def __init__(self, n_clusters, cutoff=0.05, max_iter = 500):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroids = np.zeros(n_clusters)\n",
    "        self.cutoff = cutoff\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _sos(self, arr): #sum of squares\n",
    "        return np.sum(np.power(arr,2),axis=tuple(range(1,arr.ndim)))\n",
    "\n",
    "    #CREDIT: Eli told me about this trick to calculate distances on the entire arrays\n",
    "    #        to avoid looping though every element\n",
    "    def _dist_arrs(self, X1, X2): #l2 norm between vectors in 2 arrays\n",
    "        x1_sos = np.expand_dims(self._sos(X1), axis=1)\n",
    "        x2_sos = self._sos(X2)\n",
    "        dot_sos = -2*X1.dot(X2.T)\n",
    "        sos = np.round(x1_sos+x2_sos+dot_sos,10) #round to 10 decimals to avoid negative due to machine error\n",
    "        return np.sqrt(sos)\n",
    "\n",
    "    def _dist(self,x1,x2): #l2 norm between 2 vectors\n",
    "        sq_err = np.power((x1 - x2),2)\n",
    "        sm = np.sum(sq_err)\n",
    "        return np.sqrt(sm)\n",
    "\n",
    "    def _calc_centroids(self, X, labels): #calculate new centroids as mean position of all x with same label\n",
    "        n_cents = np.array([np.mean(X[labels == i],axis=0) for i in range(self.n_clusters)])\n",
    "        n_cents[np.isnan(n_cents)] = self.centroids[np.isnan(n_cents)] #any centroids with no members remain stationary\n",
    "        return np.array([np.mean(X[labels == i],axis=0) for i in range(self.n_clusters)])\n",
    "    \n",
    "    def _nearest_centroid_arr(self, X): #calculate nearest centroid for each element in array\n",
    "        dist = self._dist_arrs(X,self.centroids)\n",
    "        labels = np.argmin(dist,axis=1) #label with index of nearest centroid\n",
    "        return labels\n",
    "\n",
    "    def _nearest_centroid(self, x):\n",
    "        min_dist = 1e100\n",
    "        label = 0\n",
    "        for i,cent in enumerate(self.centroids):\n",
    "            d = self._dist(x,cent)\n",
    "            if(d < min_dist):\n",
    "                label = i\n",
    "                min_dist = d\n",
    "        # dist = [self._dist(x,cent) for cent in self.centroids] #calculate distances to centroids\n",
    "        # label = np.argmin(dist) #label with index of closest centroid\n",
    "        return label\n",
    "\n",
    "    def fit(self,X):\n",
    "        X = np.array(X)\n",
    "        cent_idx = np.random.choice(range(len(X)), self.n_clusters, replace=False) #indices of x's that will be used as initial centroids\n",
    "        self.centroids = np.array([X[i] for i in cent_idx])\n",
    "        xlabels = self._nearest_centroid_arr(X) #label each sample with nearest centroid\n",
    "        training = True\n",
    "        cnt = 0\n",
    "        while training:\n",
    "            cnt = cnt + 1\n",
    "            n_centroids = self._calc_centroids(X, xlabels) #new centroids at mean of labeled samples\n",
    "            diff = np.max([self._dist(n_centroids[i], centroid) for i,centroid in enumerate(self.centroids)]) #distance centroids moved\n",
    "            self.centroids = n_centroids #update centroids\n",
    "            xlabels = self._nearest_centroid_arr(X) #label each sample x by nearest centroid\n",
    "            training = diff > self.cutoff #exit loop if no centroid moves by more than cutoff\n",
    "            if(cnt > self.max_iter):\n",
    "                print('Maximum Iterations',self.max_iter,'reached before cunvergence')\n",
    "                training = False\n",
    "        return self\n",
    "\n",
    "    def predict(self,x):\n",
    "        if(x.ndim > 1):\n",
    "            return self._nearest_centroid_arr(x)\n",
    "        else:\n",
    "            return self._nearest_centroid(x)"
   ]
  },
  {
   "source": [
    "## Training Models\n",
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR_PATH = '../train/'\n",
    "\n",
    "train_files = listdir(TRAIN_DIR_PATH)\n",
    "\n",
    "#CREDIT: Eli gave me the idea to use a dictionary like this rather\n",
    "#        than the messy attempt I had at just using arrays\n",
    "keys = ['wave','inf','eight','circle','beat3','beat4']\n",
    "obs_dict = {k:[] for k in keys}\n",
    "all_obs = []\n",
    "for f in train_files:\n",
    "    obs = np.genfromtxt(fname=TRAIN_DIR_PATH + f, usecols=[1,2,3,4,5,6]) #read observations from file\n",
    "    all_obs.append(obs)\n",
    "    if('wave' in f):\n",
    "        obs_dict['wave'].append(obs)\n",
    "    elif('inf' in f):\n",
    "        obs_dict['inf'].append(obs)\n",
    "    elif('eight' in f):\n",
    "        obs_dict['eight'].append(obs)\n",
    "    elif('circle' in f):\n",
    "        obs_dict['circle'].append(obs)\n",
    "    elif('beat3' in f):\n",
    "        obs_dict['beat3'].append(obs)\n",
    "    elif('beat4' in f):\n",
    "        obs_dict['beat4'].append(obs)\n",
    "all_obs = np.concatenate(all_obs)"
   ]
  },
  {
   "source": [
    "### Cluster Observations using KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates = 20 #number of hidden states\n",
    "nobs = 100 #number of observation clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clustering wave ...\n",
      "clustering inf ...\n",
      "clustering eight ...\n",
      "clustering circle ...\n",
      "clustering beat3 ...\n",
      "clustering beat4 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kmeans = MyKMeans(n_clusters=nobs).fit(all_obs) #train k-means on all observations\n",
    "\n",
    "c_obs_dict = {}\n",
    "for k in keys:\n",
    "    print('clustering',k,'...')\n",
    "    c_obs_dict[k] = [kmeans.predict(obs) for obs in obs_dict[k]] #classified/clustered observations\n",
    "print('Done!')"
   ]
  },
  {
   "source": [
    "### Pickle KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store trained kmeans in pkl because they take forever to train\n",
    "fname = 'kmeans.pkl'\n",
    "f = open(fname,'wb')\n",
    "pkl.dump(kmeans,f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "### Load Pickled KMeans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pickled kmeans file to avoid retraining models\n",
    "fname = 'kmeans.pkl'\n",
    "f = open(fname,'rb')\n",
    "kmeans = pkl.load(f)\n",
    "f.close\n",
    "\n",
    "c_obs_dict = {}\n",
    "for k in keys:\n",
    "    c_obs_dict[k] = [kmeans.predict(obs) for obs in obs_dict[k]] #classified/clustered observations"
   ]
  },
  {
   "source": [
    "### Train HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainingwave..."
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c60c38fee0a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mHMMs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mA_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi_rand\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#randomly initialize HMM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mbig_obs_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_obs_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaum_welsch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHMMs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbig_obs_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mHMMs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlikelihoods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9840533ba0a4>\u001b[0m in \u001b[0;36mbaum_welsch\u001b[1;34m(HMM, obs, N)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#HMM Filtering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#---------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_filt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#run forward filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_filt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#run backward filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmoother\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#run smoother\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6f285a5c528a>\u001b[0m in \u001b[0;36mforward_filt\u001b[1;34m(A, B, pi, obs)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mB_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB_k\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#P(j,o..) = P(i,o..)P(j|i)P(o|j)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mscale_factor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 50 #number of iterations for Baum-Welsch\n",
    "\n",
    "np.random.seed(117)\n",
    "A_rand = rand_prob_mat((nstates,nstates))\n",
    "B_rand = rand_prob_mat((nstates,nobs))\n",
    "pi_rand = rand_prob_mat((nstates,))\n",
    "\n",
    "likelihoods = {}\n",
    "HMMs = {}\n",
    "for k in keys:\n",
    "    print(\"training \",k,'...',end='',sep='')\n",
    "    HMMs[k] = [A_rand, B_rand, pi_rand] #randomly initialize HMM\n",
    "    big_obs_vect = np.concatenate(c_obs_dict[k])\n",
    "    bw = baum_welsch(HMMs[k], big_obs_vect, N)\n",
    "    HMMs[k] = bw[0:3]\n",
    "    likelihoods[k] = bw[3]\n",
    "    # for c_obs in c_obs_dict[k]: #loop through observation files\n",
    "    #     HMMs[k] = baum_welsch(HMMs[k], c_obs, N) #train HMM\n",
    "    #     print('.',end='')\n",
    "    print('done')\n",
    "    for param in HMMs[k]:\n",
    "        assert not np.any(np.isnan(param)) #error if there are any nans\n",
    "print('Done!')"
   ]
  },
  {
   "source": [
    "### Pickle Trained HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store trained HMMs in pkl because they take forever to train\n",
    "fname = 'HMMs.pkl'\n",
    "f = open(fname,'wb')\n",
    "pkl.dump(HMMs,f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "### Load Pickled HMMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "#Read pickled HMM file to avoid retraining models\n",
    "fname = 'HMMs.pkl'\n",
    "f = open(fname,'rb')\n",
    "HMMs = pkl.load(f)\n",
    "f.close"
   ]
  },
  {
   "source": [
    "## Predict Gestures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_mod_lik(HMM, obs): #likelihood that given observations came from given HMM\n",
    "    K = len(obs) #number of observations\n",
    "    A, B, pi = HMM\n",
    "    _, c = forward_filt(A, B, pi, obs) #forward filter\n",
    "    lik = -np.sum(np.log(c)) #calculate likelihood of seeing these obs under this HMM\n",
    "    return lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gesture(HMMs, obs):\n",
    "    keys = np.array(list(HMMs.keys()))\n",
    "    l = np.zeros(len(keys))\n",
    "    max_l = 0\n",
    "    pred = 'none'\n",
    "    c_obs = kmeans.predict(obs)\n",
    "    for i,k in enumerate(keys):\n",
    "        #c_obs = kmeans[k].predict(obs)\n",
    "        l[i] = obs_mod_lik(HMMs[k], c_obs)\n",
    "        if(k == 'wave'):\n",
    "            l[i] *= 0.6 #likelihood of wave artificially increased because of\n",
    "                        #its relationship with likelihood of inf\n",
    "    # arrange likelihoods and keys in descending order\n",
    "    sort_ind = l.argsort()\n",
    "    sorted_l = l[sort_ind[::-1]]\n",
    "    sorted_keys = keys[sort_ind[::-1]]\n",
    "    return [sorted_keys,sorted_l] #returns gestures in order of decreasing likelihood and associated likelihoods"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- beat3_31.txt ----\n",
      "1. beat3\t-412.87099308569594\n",
      "2. beat4\t-129239.38471487182\n",
      "3. wave\t-146468.01236039912\n",
      "\n",
      "---- beat4_31.txt ----\n",
      "1. beat4\t-697.3715606016762\n",
      "2. wave\t-237663.1866077662\n",
      "3. beat3\t-258722.3781064947\n",
      "\n",
      "---- circle31.txt ----\n",
      "1. beat4\t-168882.30119938627\n",
      "2. circle\t-198033.65725262315\n",
      "3. wave\t-260284.21891204698\n",
      "\n",
      "---- eight31.txt ----\n",
      "1. eight\t-657.6731988453323\n",
      "2. wave\t-188808.46900902365\n",
      "3. inf\t-251111.4796806891\n",
      "\n",
      "---- inf31.txt ----\n",
      "1. inf\t-1764.3464629249443\n",
      "2. beat3\t-332735.88949287566\n",
      "3. wave\t-353124.4498615668\n",
      "\n",
      "---- wave31.txt ----\n",
      "1. beat3\t-151533.0547552234\n",
      "2. beat4\t-151955.42742992318\n",
      "3. wave\t-182779.20468186736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR_PATH = '../old_test/'\n",
    "test_files = listdir(TEST_DIR_PATH)\n",
    "\n",
    "for f in test_files:\n",
    "    obs = np.genfromtxt(fname=TEST_DIR_PATH + f, usecols=[1,2,3,4,5,6]) #read observations from file\n",
    "    pred,ls = classify_gesture(HMMs, obs)\n",
    "    print('----',f,'----')\n",
    "    for i in range(3):\n",
    "        print(i+1, '. ', pred[i], '\\t', ls[i], sep='')\n",
    "    print()"
   ]
  }
 ]
}